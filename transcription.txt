 So, we will in this module we look at a practical example where PCA is used and I just like to give you a flavor of why all this is important right, why do we need to throw away some dimensions and how does it practically help ok. So, consider that we are given a large number of human images right, so this is like some faces data set or database that say one of the intelligence agencies or someone is maintaining right, so one of the government agencies or maybe other database or something like that ok. Now, each image here is 100 cross 100 that means it has 10 k dimensions right, it is a very high dimensional data ok and your job is to actually store this onto some database for a large amount of the population right, because you are collecting these images from various people. So, now we would like to represent and store this data using much fewer dimensions right and you would be really ambitious that we would want to store them using 50 to 200 dimensions right. So, you see the amount of compression that I am looking at, you have 10 k which is a big storage problem for me and I will just try to bring in doubt of 50 to 200, but I have got this is crucial data right, I do not want to store information which is not able to distinguish these faces, I should still be able to reconstruct the faces from this information right or do a law I mean minimum error reconstruction from this and that is exactly what PCA was allowing us to do right. So, now we construct a matrix of m cross 10 k, what is m? The number of samples that we have right, the number of data points that we have and each of this is of dimension 10 k ok. So, this is what matrix, what do we call this matrix, oh it is already given right, it is the x matrix, the data matrix that we always have ok. Now, each row of the matrix corresponds to one image and each image is represented using 10 k dimensions right, just to reiterate ok. Now let us see, so now what would you do, this is the original data, now you want a dimensionality reduced data right, you want store this, this is the mic work, you want this data to be represented by fewer dimensions. So, what is your solution, do PCA. So, what will you do, x transpose x right and ok, you did not get my slide at a time ok. So, we retain the top 100 dimensions corresponding to the top 100 eigenvectors of x transpose x right. So, basically we do a PCA, find the 100, find all the eigenvectors of x transpose x and then just retain the top 100 of those ok. Now, what is the dimension of each of these eigenvectors, should be straight forward but take your time, it is early morning. 10 k. 10 k right ok. So, now can you think of a physical interpretation of this. So, what are you trying to do, you are trying to store faces ok and now you have come up with these basis vectors right, which are the eigenvectors and each of them is also 10 k, which is the same as the dimensions of your faces ok. Can you think of a physical interpretation of what is happening here, none of you went through the slides except perhaps you are I do not know, just think about it. So, what you are trying to do is, you are trying to represent any possible face in your database right, using a linear combination of some vectors ok. Now, these vectors should have some interpretation right, it should be connected to faces in some way, otherwise how will you construct a face from taking a linear combination of some random vectors, do you get the point. So, can you think of each of these 10 k dimensional vectors, which is the same as a dimension of your original data, as a face and try to plot it. Can you try to do that, at least it makes sense because it is 10 k dimensional ok, that is the same as what your image size was, I could just arrange these 10 k dimension as 100 cross 100 and try to plot it ok. So, let us see if you do that what happens ok, you can convert each eigenvector into a 100 cross 100 matrix and treat it as an image and let us see what we get, this is what we get. So, this is the top 16 eigenvectors that I have plotted. Now, can you tell me a physical interpretation of this, this is the basis for constructing any face in your database right, the what you are trying to say is that all the faces that you have in your database or in the world, you can combine them by looking at these elementary face structures right, which are your bases and then you could scale them up by using these alphas, you will be multiplied them with a certain alpha right and when you combine them, you will get any base any face that you had in your original database. Does the physical interpretation make sense, how many of you get this? So, that is what is happening here right. So, you have constructed this basis now ok, I will come to that later. So, these images are actually called Eigen faces and they form a basis for representing any face in your database ok. In other words, we can now represent a given image as a linear combination of these Eigen faces. So, this is my original image ok, I want to reconstruct it. Now, I will use 16 or 25 of these Eigen faces, what do you think would happen? You will get some face which has some error, there is some error in reconstructing this face ok. So, let us see what we get right. So, I am using only one basis vector and I found out this alpha 1 I, how did I find it out? Dot product of the face vector with the basis vector ok. Now, if instead of 1 I take 2 ok, you see I took these two basis vectors, scaled them with the corresponding alpha coordinates and added them up right and I am trying to get some face, but it still does not look anywhere close to the original face. So, that means, the dash is very high. The reconstruction error is very high that means, I have still dropped some of the important dimensions right, still drop some of the modern Eigen vectors right. So, the value of k which is the top k Eigen vectors is something that I need to take care of right, it should be in a way I can always construct the reconstruct, I can always compute the reconstruction error here right. How will you compute the reconstruction error? Take the square error. Take the square error between the original image and this second image that you see here right. So, you will take the square error between this and this and you will end up with a number which is not acceptable right. Now, what I will do is, I will go further, I have taken 4 ok, still not quite there, but I can see a shape emerging right. I go to 8 things become better, since you already know what the original face is at least you can make sense of it and by the time I read 16, I am almost there right, at least I can recognize the face, it is probably losing out some subtle things in the face, but I can recognize it. Now, how many of you appreciate what is happening here? Yeah, of course. Now, what is happening here? So, think in terms of a practical application right, what have you done, what have you been able to achieve, how many basis vectors were you able to store or did you store? 16 that means, 16 into 10k values ok and suppose you had a million images in your database, how many would you require to store? . Ok, wait let us, let us do it step wise, forget about PCA, if you had a million images in your database and each of them have 10k dimensions, how much storage did you need? Million into 10k ok, floating point values ok. Now, with if I say 16, 16 maybe too ambitious, maybe later on I will say 50 or 100 is ok, but let us say 16 ok, then how much data do you need to store? 16 to 10k. 16 to 10k and you can reconstruct any face. . The alphas need to be stored right. So, for every image instead of storing 10k dimensions, we will just store the 16 alphas right and you can see that even if I go to 100 it is still manageable, instead of 10k I am going to just store 100 alphas right and as I go to 100 what would happen? The reconstruction error would become even lesser ok. So, is that is the intuition clear? ok. So, this eigenvector storage is a one time storage, we are going to store these k eigenvectors each of them is 10k dimensional and if k is 100 or 200 we do not really care because the original data was very large right and for each image we just need to store these alpha values k of them right. So, for each image instead of 10k we will just store 100 to 200 alpha values and of course, this is significantly reduced right. So, this is why we need to do all this right and what is the other advantage of doing this? Can you think of something else? So, what is PCA actually allowing you to do? If you again think of it and not I mean subtract the math out just think of it in terms of physical interpretation it what is it that it is allowing you to do? If you had to say it in English what is it allowing you to do? Compression, compression is a loaded word can you just spell it out for me what does compression mean actually? . Right, so it is storing all the relevant information in the image and discarding all the irrelevant information right. Now, this also ensures that if you have multiple images of the person then what would happen? If you have taken these images under different lighting conditions or maybe that person has applied some makeup or something like that right, what would happen? In the original space the 10k dimensional space what would happen to these images? They will be very far from each other right because the lighting conditions have changed you see a dark person instead of a light person or something like that right and now because PCA has helped you to throw away these dimensions right. Maybe the exact terminology which I am using maybe lighting conditions do not do it, but you can imagine that there would be something right that suppose there is some element which is causing the image to look slightly different, but that is not the important information right. So, that would get discarded off and only the relevant information would stay right. So, then multiple images of the same person which were dash in the original space would come dash in the new space, far in the original space space would come closer in the new space right. So, this is what compression helps you do right. So, this is what you want to learn, you want to learn the important characteristics of your original data and that is what PCA allows you to do okay. . . . . . . . . . . . . . . .